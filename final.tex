\documentclass{mini}
\usepackage[utf8]{inputenc}
\usepackage{fancyvrb}
\usepackage{amsmath}
\usepackage{subfig}
\newcommand{\degree}{\ensuremath{^\circ}}
\newcommand{\hilight}[1]{\colorbox{yellow}{#1}}
\linespread{1.5}
\DeclareUnicodeCharacter{00A0}{~}

%------------------------------------------------------------------------------%
\title{Analiza możliwości wykorzystania w~algorytmie CMA-ES wiedzy o~ograniczeniach kostkowych}

\author{inż. Robert Jakubowski}
\supervisor{dr hab. inż. Jarosław Arabas prof. nzw. PW}
\type{magisters}
\monthyear{Maj 2016}
\date{\today}
\album{237545}
%------------------------------------------------------------------------------%
\begin{document}

\maketitle

\pagebreak
\thispagestyle{empty}

\openup -0.2em %make it fit on one page
\tableofcontents
\openup 0.2em %make it fit on one page

\thispagestyle{empty}
\raggedbottom
\pagebreak


\section{Streszczenie}

\pagebreak

\section{Wstęp}

\subsection{Cel pracy}

\pagebreak

\section{Techniki uwzględniania ograniczeń}
Niektóre problemy optymalizacyjne posiadają ograniczenia. Szukając rozwiązania należy zapewnić, że będzie ono dopuszczalne. Bazując na ------przypis------ techniki uwzględniania ograniczeń można podzielić w następujący sposób:
\begin{itemize}[noitemsep]
\item definicja przestrzeni przeszukiwań - zapewnienie, że podczas krzyżowań, mutacji i innych zmian punktów, żaden z punktów nie wypadnie poza przestrzeń przeszukiwań,
\item modyfikacja funkcji celu - zmienienie funkcji celu tak, aby funkcja celu dla punktów spoza ograniczeń zwracały gorsze wyniki,
\item transformacja rozwiązań - punkty, które są poza ograniczeniami zostają zamieniane na punkty, które znajdują się w ograniczeniach.
\end{itemize}

W tej pracy skupiono się na transformacji rozwiązań.

\subsection{Transformacje rozwiązań} \label{transformacje}
Istnieje wiele technik transformacji rozwiązań spoza ograniczeń na dopuszczalne. W~kolejnych podrozdziałach znajdują się metody transformacji rozwiązań, które zostały zbadane. Każda z technik jest opisana słownie oraz pseudokodem. Opis słowny zawiera wyjaśnienie, co się dzieje z punktem, który znalazł się poza ograniczeniem. W~pseudokodzie zastosowano następujące oznaczenia:
\begin{itemize}[noitemsep]
\item $x$ - punkt, który poddajemy naprawie
\item $x'$ - rodzic punktu $x$, czyli z punktu $x'$ z zadanym rozkładem został wygenerowany punkt $x$
\item $x(i)$ - wartość $i$-tej współrzędnej punktu $x$
\item $lb$ - ograniczenie dolne
\item $ub$ - ograniczenie górne
\end{itemize}

\subsubsection{Metoda klasyczna}
Nowy punkt zostaje odrzucony i wraca do poprzedniej pozycji.

\begin{Verbatim}[baselinestretch=1.1]
	x = x'
\end{Verbatim}


\subsubsection{Rzutowanie}
Punkt jest transformowany do najbliższego punktu, który spełnia ograniczenie. Oznacza to, że dla każdej współrzędnej sprawdzany jest warunek zawierania się w ograniczeniach. Dla współrzędnych, dla których nie jest on spełniony, wartość jest zamieniana na wartość ograniczenia (dolnego lub górnego), które jest najbliżej.

\begin{Verbatim}[baselinestretch=1.1]
	dla każdej współrzędnej i
		jeżeli lb(i) > x(i)
			x(i) = lb(i)
		jeżeli ub(i) < x(i)
			x(i) = ub(i)
\end{Verbatim}

\subsubsection{Reinicjacja}
Punkt jest przenoszony do pozycji początkowej. W tej pracy był to jednocześnie środek układu współrzędnych oraz środek symetrii ograniczeń.

\begin{Verbatim}[baselinestretch=1.1]
	x = x0
\end{Verbatim}


\subsubsection{Odbicie}
Dla każdej współrzędnej sprawdzane są warunki na ograniczenie. W przapadku współrzędnych, na których punkt jest poza ograniczeniem, wartość punktu tej współrzędnej jest symetrycznie odbita względem ograniczenia, którego warunek został złamany.

\begin{Verbatim}[baselinestretch=1.1]
	dla każdej współrzędnej i
		jeżeli lb(i) > x(i)
			x(i) = x(i) + 2 * (lb(i) - x(i))
		jeżeli ub(i) < x(i)
			x(i) = x(i) - 2 * (x(i) - ub(i))
\end{Verbatim}

\subsubsection{Próbkowanie}
Punkt jest powtórnie generowany do momentu, aż spełni ograniczenie kostkowe.

\begin{Verbatim}[baselinestretch=1.1]
	dopóki !w_ograniczeniach(x)
		x = losuj(x')
\end{Verbatim}

\subsubsection{Zawijanie}
Dla każdej współrzędnej sprawdzane są warunki na ograniczenie. W przypadku współrzędnych, na których punkt jest poza ograniczeniem, różnica, pomiędzy ograniczeniem a wartością współrzędnej punktu, jest zapamiętywana. Tę różnicę odkładamy na przeciwległym ograniczeniu po stronie, która jest wewnątrz ograniczenia. W tym miejscu znajduje się nowa wartość współrzędnej punktu. W intuicyjny sposób można to wyjaśnić tak, że dla punktów nie ma ograniczeń, a przestrzeń przeszukiwań po każdym wymiarze jest jakby "zawinięta".

\begin{Verbatim}[baselinestretch=1.1]
	dla każdej współrzędnej i
		jeżeli lb(i) > x(i)
			x(i) = ub(i) - (lb(i) - x(i))
		jeżeli ub(i) < x(i)
			x(i) = lb(i) + (x(i) - ub(i))
\end{Verbatim}

\subsection{Błądzenie przypadkowe} \label{bladzenie}

Można się spodziewać, że algorytm CMA-ES dla funkcji stałej będzie zachowywał się analogicznie do błądzenia przypadkowego. Takie założenie skłoniło autora, żeby zbadać błądzenie przypadkowe z ograniczeniami. Błądzenie przypadkowe jest algorytmem dużo prostszym, niż CMA-ES, więc umożliwia szybsze testowanie i wyciąganie wniosków.\\
Niech $ X_1, X_2, ... $ będą niezależnymi n-wymiarowymi zmiennymi losowymi o wartości oczekiwanej równej $ [0]^n $. Błądzeniem przypadkowym nazywamy sekwencję punktów $S_1$, $S_2$, ..., takich że:

\begin{equation}
S_1 = X_1, S_i=S_{i-1}+X_i
\end{equation}

Zmienne losowe mogą być realizowane w różny sposób. Mogą to być wektory o rozkładzie normalnym, jednostajnym lub innym.

\subsection{Metoda przeprowadzania testów}
Celem testów jest zaobserwowanie, jaki wpływ na symulację ma metoda uwzględniania ograniczeń. Wybrano 2 cechy, które miały być przetestowane:
\begin{enumerate}
\item Wartość oczekiwana generowanego punktu (razem z poprawą).
\item Rozkład prawdopodobieństwa generowanych punktów.
\end{enumerate}

Sposoby testowania obu cech są opisane w kolejnych podrozdziałach.

\subsubsection{Wartość oczekiwana generowanego punktu}
Błądzenie przypadkowe jest dobrze określone. W tym algorytmie wartością oczekiwaną losowanej liczby $X_i$ jest 0. Implikuje to fakt, że wartością oczekiwaną $S_i$ jest $S_{i-1}$. Dodanie ograniczeń kostkowych powoduje, że nadal losojuemy liczbę $X_i$, której wartość oczekiwana jest równa 0. Dodajemy jednak metody naprawy. W związku z tym nie zawsze wartością oczekiwaną $S_i$ jest $S_{i-1}$.\\
Dla każdego punktu $p$ przestrzeni przeszukiwań można przypisać wektor $\overrightarrow{d}$. Początek $\overrightarrow{d}$ znajduje się w $p$. Załóżmy teraz, że $p$ jest punktem $S_i$ pewnego błądzenia przypadkowego z ograniczeniami. Koniec wektora $\overrightarrow{d}$ znajduje się w wartości oczekiwanej punktu $S_{i+1}$.\\
Wykres, który będzie zawierał wektory $\overrightarrow{d}$, pozwoli na określenie charakterystyki przemieszczania się punktów w poszczególnych częściach przestrzeni przeszukiwań.\\

\subsubsection*{Implementacja}
Do wizualizacji wybrano symulację, w której punkty posiadają 2 wymiary. Pozwala to na przejrzystą wizualizację wektorów dla każdego z punktów. Pokazuje też zachowanie się wektorów w rogach - punktach, które są blisko ograniczeń.\\
Dla obu wymiarów wybrano kilkanaście, równoodległych punktów. W skład tych punktów wchodziły też punkty znajdujące się na ograniczeniach. Następnie stworzono 2 pętle po tych punktach (jedna zagnieżdżona w drugiej). W ten sposób uzyskano regularnie rozłożone punkty w dwóch wymiarach.\\
Dla każdego z tych punktów przeprowadzono następujące obliczenia. 1000 razy zasymulowano krok algorytmu błądzenia przypadkowego (z rozkładem normalnym) z odpowiednią naprawą. Otrzymano tysiąc punktów, z których następnie obliczono średnią arytmetyczną. Tak uzyskany punkt potraktowano jako wartość oczekiwaną i wyrysowano wektor $\overrightarrow{d}$.

\subsubsection{Rozkład prawdopodobieństwa generowanych punktów}
Punkty w błądzeniu przypadkowym poruszają się chaotycznie. Ta losowość jest częściowo uporządkowana na ograniczeniach. W zależności od metody naprawy, punkty zachowują się inaczej, a rozkład prawdopodobieństwa jest zniekształcony. Autor postanowił przyjrzeć się jak wygląda rozkład prawdopodobieństwa wystąpienia punktu z~perspektywy całej symulacji.\\
Dla każdej metody naprawy opisanej w \ref{transformacje} jednokrotnie uruchomiono algorytm błądzenia przypadkowego jednego punktu. Symulowano oddzielnie również błądzenie z rozkładem normalnym oraz jenostajnym na przedziale $[-0.5; 0.5]$ (przedział co najmniej kilkukrotnie krótszy od ograniczeń kostkowych). Po każdej iteracji nowo wygenerowany punkt zapisywano w tablicy. Z tak przygotowanej tablicy generowano histogram.

\subsubsection*{Implementacja}
Dla każdej z metod naprawy  przygotowano oddzielny skrypt. Pseudokod skryptu zamiszczono poniżej.
\begin{Verbatim}[baselinestretch=1.1]
	x - błądzący punkt
	punkty - tablica wszystkich położeń punktu x
	iteracje - liczba iteracji podana jako parametr
	i = 0
	dopóki i < iteracje
		d = wektor wylosowany z zadanym rozkładem
		x = x + d
		jeżeli x jest poza ograniczeniem
			popraw x
		dodaj x do tablicy punkty
		i = i + 1
\end{Verbatim}

\subsection{Wyniki testów}
Wykresy zamieszczone w tym podrozdziale są histogramami wystąpień punktu. Symulacje skryptów były uruchamiane z liczbą iteracji wynoszącą 2-100 milionów. Szerokość przedziałów na wykresach jest różna i była dobierana tak, aby jak najlepiej przedstawić interesujące fakty.\\
Wszystkie wykresy, które pokazdują wyniki błądzenia przypadkowego w dwóch wymiarach zostały przetworzone w następujący sposób: otrzymane wyniki symulacji powielono poprzez symetrzyczne odbicie ich względem osi X, osi Y oraz początku układu współrzędnych. Wykonano ten zabieg, aby zwiększyć dokładność wyników. Nie wpływają one na badania, poniważ punkty 0 są środkami ograniczeń obu osi, a z perspektywy algorytmu nie ma znaczenia, w której ćwiartce znajduje się aktualnie punkt.

\subsubsection*{Metoda klasyczna}
Histogramy pokazują, że punkt z większym prawdopodobieństwem występuje przy ograniczeniach. Takie zachowanie można uznać za intuicyjne, ponieważ potomkowie punktów blisko granicy mogą "wypaść" poza ograniczenie. Po naprawie dziecko będzie tym samym punktem. Symulacja dwuwymiarowa nie pokazuje tego tak dokładnie, natomiast również można dostrzec ten efekt.

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{conservative2dprzesuniecie}
\caption{Wartość oczekiwana punktu}
\end{figure}

\begin{figure}[H]
\centering
\subfloat[Generowanie z rozkładem normalnym]{\includegraphics[width=0.45\textwidth]{c_n_20M_1__5_5_2}}
\quad
\subfloat[Generowanie z rozkładem jednostajnym]{\includegraphics[width=0.45\textwidth]{c_j_2M_1__5_5}}
\caption{Rozkład prawdopodobieństwa punktów; 1 wymiar}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{c_n_10M_2__20_20__10_10_4}
\caption{Rozkład prawdopodobieństwa punktów; 2 wymiary; generowanie z rozkładem normalnym}
\end{figure}

\subsubsection*{Rzutowanie}
Zgodnie z przewidywaniami największe prawdopodobieństwo wystąpienia punktu jest na ograniczeniu, co bardzo dobrze obrazuje rysunek \ref{bladzenie:rzutowanie2d}. Na rysunkach \ref{bladzenie:rzutowanie1dn} i \ref{bladzenie:rzutowanie1dj} zakres osi~Y został zmniejszony do odpowiednio $[30000;60000]$ oraz $[70000;140000]$, aby uwypuklić interesujące efekty. W związku z tym, zostały "ucięte" wartości przedziałów brzegowych. Warto zwrócić uwagę na niespodziewane zjawisko - wartości przedziałów blisko ograniczeń są mniejsze, niż pozostałe. Oznacza to, że punkty w tych przedziałach występują z mniejszym prawdopodobieństwem, niż pozostałe. Wydawać by się mogło, że powinno być inaczej, ponieważ podczas symulacji punkty często są rzutowane na ograniczenie. Dodatkowo, w rozkładzie jednostajnym widać, że istnieją przedziały, w~których punkty występują z większym prawdopodobieństwem. Autorowi nie udało się formalnie uzasadnić tego zjawiska. Intuicja prowadzi do hipotezy, że punkty, które znalazły się poza ograniczeniem, bez naprawy po kilku iteracjach zapełniałyby "dołek". Naprawa natomiast sprawia, że cała dalsza symulacja zostaje niejako przesunięta. W~ten sposób powstaje "górka" w~rozkładzie jednostajnym. Z charakterystyki rozkładu normalnego może brać się fakt, iż owa górka w nim nie występuje.

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{projection2dprzesuniecie}
\caption{Wartość oczekiwana punktu}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{p_n_50M_1__5_5}
\caption{Rozkład prawdopodobieństwa punktów; 1 wymiar; generowanie z rozkładem normalnym}
\label{bladzenie:rzutowanie1dn}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{p_j_100M_1__3_3}
\caption{Rozkład prawdopodobieństwa punktów; 1 wymiar; generowanie z rozkładem jednostajnym}
\label{bladzenie:rzutowanie1dj}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{p_n_10M_2__20_20__10_10_4_2}
\caption{Rozkład prawdopodobieństwa punktów; 2 wymiary; generowanie z rozkładem normalnym}
\label{bladzenie:rzutowanie2d}
\end{figure}

\subsubsection*{Reinicjacja}
Na pierwszy rzut oka wykresy nie ukazują nic nadzwyczajnego, ponieważ łatwo zauważyć pik związany z reinicjacją oraz wartości histogramu malejące wraz ze zbliżaniem się do ograniczeń. Warto jednak zwrócić uwagę na 2 fakty.\\
Pierwsze spostrzeżenie, to kształt histogramu. Zarówno dla rozkładu jednostajnego, jak i normalnego w jednym wymiarze jest on stożkowy. Łuk można zauważyć tylko blisko punktu środkowego, w pozostałej części spadek jest liniowy. Brakuje charakterystycznego, gaussowskiego przegięcia. Sytuacja jest ciekawsza, gdy występuje więcej wymiarów. Widać wówczas przegięcie. Dokładniejsze badania pokazały, że przegięcie nie występuje tylko na jednym wymiarze - tym, który jest relatywnie najkrótszy. Celowo jest użyte słowo relatywnie, ponieważ z~perspektywy błądzenia przypadkowego i~rozkładu normalnego trzeba brać pod uwagę parametr $\sigma$. Wymiary o małym $\sigma$ będą relatywnie dłuższe od tych z dużym $\sigma$, ponieważ błądzenie będzie wykonywało mniejsze kroki.\\
Na rysunku \ref{bladzenie:probkowanie1dj} można dostrzec też dwa uskoki, które związane są z pikiem w punkcie~$0$ oraz charakterystyką rozkładu.

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{reinitialization2dprzesuniecie}
\caption{Wartość oczekiwana punktu}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{ri_n_20M_1__5_5}
\caption{Rozkład prawdopodobieństwa punktów; 1 wymiar; generowanie z rozkładem normalnym}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{ri_j_20M_1__3_3}
\caption{Rozkład prawdopodobieństwa punktów; 1 wymiar; generowanie z rozkładem jednostajnym}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{ri_n_10M_2__20_20__10_10_4}
\caption{Rozkład prawdopodobieństwa punktów; 2 wymiary; generowanie z rozkładem normalnym}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{ri_n_10M_2__20_20__10_10_4_1D}
\caption{Rozkład prawdopodobieństwa punktów; 2 wymiary; generowanie z rozkładem normalnym; oddzielne histogramy dla obu wymiarów}
\end{figure}

\subsubsection*{Próbkowanie}
Ten rozkład charakteryzuje się spadkiem wartości prawdopodobieństwa wraz ze zbliżaniem się do ograniczenia. Punkty, które wypadłyby poza ograniczenia oraz ich potomkowie są przesuwane w kierunku środka przedziału.

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{sampling2dprzesuniecie}
\caption{Wartość oczekiwana punktu}
\end{figure}

\begin{figure}[H]
\centering
\subfloat[Generowanie z rozkładem normalnym]{\includegraphics[width=0.45\textwidth]{s_n_10M_1__5_5}}
\quad
\subfloat[Generowanie z rozkładem jednostajnym]{\includegraphics[width=0.45\textwidth]{s_j_20M_1__3_3}}
\caption{Rozkład prawdopodobieństwa punktów; 1 wymiar}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{s_n_10M_2__20_20__10_10_4_2}
\caption{Rozkład prawdopodobieństwa punktów; 2 wymiary; generowanie z rozkładem normalnym}
\end{figure}

\subsubsection*{Zawijanie}
Wyobraźmy sobie, że naszą symulację przeprowadzamy bez ograniczeń, następnie wykres z wynikami tniemy w równych odstępach wzdłuż każdej osi. Na koniec tak pocięte części łączymy w jeden wykres. Takie zachowanie symuluje zawijanie. W związku z tym nie dziwi fakt, że poniższe histogramy przedstawiają rozkład jednostajny z szumem.

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{wrapping2dprzesuniecie}
\caption{Wartość oczekiwana punktu}
\end{figure}

\begin{figure}[H]
\centering
\subfloat[1 wymiar; generowanie z rozkładem normalnym]{\includegraphics[width=0.45\textwidth]{w_n_10M_2__20_20__10_10_4_2}}
\quad
\subfloat[2 wymiary; generowanie z rozkładem jednostajnym]{\includegraphics[width=0.45\textwidth]{w_j_20M_1__3_3}}
\caption{Rozkład prawdopodobieństwa punktów;}
\end{figure}

\subsubsection*{Odbicie}
Podobnie jak zawijanie, odbicie zwraca histogram rozkładu normalnego z szumem. W~tej sytuacji nieco trudniej o analogię, lecz po chwili zastanowienia nie dziwi kształt poniższych wykresów.

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{reflection2dprzesuniecie}
\caption{Wartość oczekiwana punktu}
\end{figure}

\begin{figure}[H]
\centering
\subfloat[1 wymiar; generowanie z rozkładem normalnym]{\includegraphics[width=0.45\textwidth]{rf_n_10M_2__20_20__10_10_4_2}}
\quad
\subfloat[2 wymiary; generowanie z rozkładem jednostajnym]{\includegraphics[width=0.45\textwidth]{rf_j_100M_1__3_3}}
\caption{Rozkład prawdopodobieństwa punktów;}
\end{figure}

\subsection{Test przesunięć}
Dodatkowo został wykonany test, w którym badano wartość oczekiwaną przesunięcia punktu w kolejnej iteracji (razem z naprawą). Dla każdej z metod wyglądał on podobnie:

Jedynie dla zawijania oraz reinicjacji długości wektorów były znacząco większe. W pozostałych zmieniał się kierunek oraz długość wektora. Były to jednak zmiany wynikające pośrednio z pokazanych wcześniej histogramów.

\subsection{Wnioski}
Techniki uwzględniania ograniczeń mają swoje charakterystyczne cechy. W większości metod zachowanie pojedynczej cząsteczki przekłada się na globalne zachowanie całego algorytmu. Testy pokazały cechy, które nie były oczywiste podczas prostej analizy algorytmów.\\
Z perspektywy algorytmu CMA-ES warto zwrócić uwagę przede wszystkim na metody zawijania i reinicjacji. W tych metodach wartości oczekiwane punktów znajdują się daleko od rodzica. Oznacza to, że w algorytmie CMA-ES macierz kowariancji może znacząco się zniekształcać.

\pagebreak

\section{Metodyka testowania metod optymalizacji globalnej}
Funkcje posiadają charakterystyczne cechy, m.in. monotoniczność, liczba ekstremów, ciągłość. Metody optymalizacji globalnej również posiadają swoje charakterystyczne własności. W zależności od przypadku, wymagania stawiane algorytmowi mogą być różne: szybkości działania, przeszukanie całej przestrzeni, czy też dokładne określenie ekstremum. Połączenie tego wszystkiego powoduje, że trudno jednoznacznie okreslić, która metoda optymalizacji jest najlepsza. Warto jednak posiadać wiedzę w których przypadkach metoda przynosi lepsze rezultaty.\\
Zapewne zgodziliby się z tym stwierzeniem twócy konkursów takich jak CEC, czy BBOB. Tworzą oni szereg funkcji benchmarkowych, które mają na celu emiprycznie pokazać silne i słabe strony algorytmów w konkretnych sytuacjach.

\subsection{Funkcje benchmarkowe}
Funkcje benchmarkowe, to złożone furnkcje. Skomplikowanie polega między innymi na wielu ekstremach funkcji lub specyficznym umieszczeniu minimum globalnego. Dzięki nim można porównać algorytmy.

\subsection{Techniki porównywania wyników}

\pagebreak

\section{Wpływ technik na efektywność CMA-ES}

\subsection{Algorytm CMA-ES}
Klasyczne algorytmy ewolucyjne nie dostosowują się do charakterystyki optymalizowanej funkcji. W większości z nich, podczas generowania punktów, rozkład prawdopodobieństwa jest stały. Z tego faktu wynika problem ustalenia parametrów przeszukiwania. Na przeciw temu wychodzi algorytm CMA-ES, który w swej idei ma dopasowywać się do badanej funkcji.\\
Rozwinięcie akronimu CME-ES podpowiada, w jaki sposób jest to realizowane: Covariance Matrix Adaptation - Evolution Strategy (adaptacja macierzy kowariancji -~strategia ewolucyjna). Jest to alorytm oparty na tehcnikach ewolucyjnych, natomiast punkty losowane są na podstawie macierzy kowariancji, która jest w każdej iteracji dostosowywana do aktualnej sytuacji przeszukiwań.

\subsubsection{Opis algorytmu}
Zgodnie z tym, co zostało ustalone na początku tego rozdziału, algorytm CMA-ES jest podobny do klasycznych algorytmów ewolucyjnych. Inny jest sposób generowania punktów. Można to zauważyć w poniższym, ogólnym pseudokodzie opisującym algorytm CMA-ES:
\begin{Verbatim}[baselinestretch=1.1]
    inicjalizuj zmienne
    dopóki nie są spełnione warunki stopu:
        generuj punkty
        oblicz wartości punktów
        przeprowadź selekcję punktów
        oblicz wartość oczekiwaną rozkładu
        oblicz długość kroku
        oblicz macierz kowariancji
\end{Verbatim}
Ten pseudokod zaiwera kilka pojęć, które nie zostały jeszcze omówione. Ich opis znajduje się w kolejnych podrozdziałach. Nie jest on wyczerpujący. Aby dokładnie zgłębić algorytm CMA-ES oraz zapoznać się z formalnym opisem poszczególnych fragmentów warto przeczytać ------przypis------

\subsubsection*{Macierz kowariancji}
Pierwszą zmienną, która będzie opisana jest macierz kowariancji. To ona jest trzonem, a wszystkie kolejne z niej wypływają. Tak jak napisano we wstępie do tego rozdziału, algoyrt CMA-ES posiada inny sposób generowania punktów, niż klasyczne algorytmy ewolucyjne. Wszystkie punkty są generowanie poprzez losowanie na podstawie macierzy kowariancji. Oznacza to, że nie istnieje bezpośredni związek pomiędzy punktami - w~klasycznych algorytmach ewolucyjych zazwyczaj można było mówić o rodzicach nowo wygenerowanego punktu.\\
Rola dotychczas wygenerowanych punktów jest jednak istotna w algorytmie CMA-ES. Mają one wpływ na adaptację macierzy kowariancji. Odbywa się to w następujący sposób: z populacji punktów wybiera się $\mu$ punktów (zazwyczaj połowę), która zwraca lepsze wartości funkcji celu. Następnie przypisuje się im wagi - w zależności od tego, jak dobry jest punkt. Na postawie tych danych wyznaczana jest empiryczna macierz kowariancji.

\subsubsection*{Wartość oczekiwana rozkładu}
Punkty generowane są na podstawie rozkładu określonego macierzą kowariancji wokół wartości oczekiwanej. Sama nie mówi jednak nic o wartości oczekiwanej. Wymaga to oddzielnego wyznaczenia tej wartości.\\
Podobnie, jak w macierzy kowariancji wybiera się $\mu$ punktów. Dla tych punktów wybiera się średnią ważoną. Ta średnia jest nową wartością oczekiwaną.

\subsubsection*{Długość kroku}
Generowanie punktów z macierzy kowariancji pozwala na skalowanie tego rozkładu przez dodanie jednego parametru. Ten parametr jest nazywany długością kroku. W~algorytmie CMA-ES jest na podstawie kierunku przemieszczania się wartości oczekiwanej rozkładu. Długość kroku zwiększa się, gdy wartość oczekiwana przemieszcza się w tym samym kierunku Skraca natomiast, gdy kierunek jest zmienny.

\subsubsection*{Generowanie punktów}
Punkty są generowane poprzez wylosowanie wektora liczb na postawie macierzy kowariancji. Wylosowany wektor jest skalowny przez długość kroku, a na koniec jest przesuwany o wektor równy wartości oczekiwanej rozkładu.

\subsubsection*{Selekcja punktów}
Selekcja punktów, to wybór $\mu$ punktów spośród całej populacji. Wybierane są te punkty, które mają lepszą wartość funkcji. Punktom przypisywane są również wagi wykorzystywane do wyznaczania wartości oczekiwanej.

\subsection{CEC 2013}
- wybrane kilka funkcji\\
- kilkukrotne powtórzenie testów\\
- porównanie dokładności wyników i liczby iteracji
Zgodnie z założeniami poczynionymi w rozdziale \ref{bladzenie} testy algorytmu CMA-ES powinny przynieść rezultaty zbliżone do testów błądzenia przypadkowego. Do przeprowadzania testów została użyta biblioteka przygotowana przez Nikolausa Hansena, współautora algorytmu CMA-ES. Podobnie, jak w przypadku błądzenia przypadkowego, wykorzystano implementację w języku MATLAB ---przypis---.

\pagebreak

\section{Podsumowanie}

\subsection{Wyniki}

\subsection{Możliwości rozwoju}

\pagebreak

\begin{thebibliography}{9}

\end{thebibliography}

\makestatement

\end{document}
